# Methodology {#methods}

<!-- Required to number equations in HTML files -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>

## Data Preprocessing   
Standard text data cleaning procedure was followed for the preprocessing of the review corpus: all characters were transformed to lowercase, punctuations and numbers were removed, stop words such as “I”, “me”, “she”, “is” (see Appendix: stopwords for the full list) were also removed, whitespaces were stripped and trimmed, and all words were stemmed using Porter stemming algorithm (implemented in R through the stemDocument function in the tm package).  

## N-Gramming  
One optional yet particularly helpful preprocessing step was n-gramming. It captures word sequences that are better perceived or carry more meaningful information as a whole. For example, “White House” should be perceived as a single token instead of separately as “white” and “house”, and similarly “President of the United States” makes more sense as a whole. N-gramming is especially helpful when identifying word sequences that are specific to the context of the corpus. In the previous example, it would be crucial to identify “White House” and “President of the United States” as n-grams in a corpus consists of political blogs. In our case where we have a corpus of restaurant reviews, we can expect to see context-specific n-grams such as “Mac ‘n Cheese”, “highly recommend”, “great service”, and even “can’t wait to go back”.  

We adopted the probabilistic approach for identifying these n-grams where the conditional probability of seeing the ith word given the (i-1)th word. The n-gramming implementation code was modified based on the NGramming Processing Script (c) by Teague Henry. The procedure in this code is for each consecutive bigram sequence (word1, word2):  

1. Calculate count1 = number of appearances of word1 in the corpus;    
2. Calculate prop2 = appearance rate of word2 in all non-word1 words in the corpus;    
3. Calculate count_bi = number of appearance of bigram word1_word2 in the corpus;    
4. Compute p_value = Pr(N  count_bi) where N is the total number of consecutive co-occurrences of word1_word2, N ~ Binomial(count1, prop2);    
5. If p-value < 0.01, then we reject the null hypothesis that the co-occurrences happened by random and identify word1_word2 as a meaningful bigram.    

The above procedure is repeated again after first run to identify trigrams and larger n-grams.  

In terms of the cutoff threshold for identifying meaningful n-grams, both count and p-values were considered. While p-value cutoff has comparatively more consistent performance, it alone would include bigrams with neglectable occurrences (eg. appeared only 2 times in the entire corpus) and thus contribute minimal information. As a result, a hybrid cutoff using both a p-value cutoff of 0.01 and empirically-set count cutoffs of 100 for bigrams and 40 for trigrams was adopted for our corpus.  

All identified n-grams will be replaced by an integrated token of the original words in the corpus, where bigrams are connected with “_” in between and trigrams with “.”. For example, after all pre-processing steps and n-gramming, “White House” would become “white_hous”, and “Mac ‘n Cheese” would be “mac_n.chees.”  

## Topic Extraction   
### Latent Dirichlet Allocation (LDA)  
To identify common topics in our corpus, we will first experiment with the topic modeling approach, or more specifically Latent Dirichlet Allocation (LDA). LDA is a Bayesian generative topic model based on the assumption that each document, as a collection of words, is a mixture of a certain number of topics, and that the occurrence of each word in that document can be attributed to one of its topics. In terms of denotations, the entire corpus is a set of documents $\{D_1, ..., Dm\}$, and the words within a document are denoted as $D_i=\{w_{i1},...,w_{in_i}\}$, $w_ij \in W$ where W is a finite vocabulary set of size $V$. Suppose we assume the entire corpus is a mixture of $K$ topics, the data generative process with LDA is as follows:

1. For each topic $k \in \{1,...,K\}$,  
   a. Draw a topic-word proportion vector $\phi_k \sim Dirichlet(\alpha)$  
2. For each document $D_i$,
   a. Draw a document-topic proportion vector $\theta_i \sim Dirichlet(\beta)$  
   b. For each word $w_{ij}$,  
      i. Draw a topic assignment $z_j \sim Multinomial(\theta_i), z_j \in \{1,...,K\}$  
      ii. Draw a word from this topic $w_{ij} \sim Multinomial(\phi_k), w_{ij} \in \{1,...,V\}$   
      
      
```{r LDA, fig.cap="LDA data generative process (Wikipedia)", echo=FALSE}
include_graphics(path = "figure/LDA.png")
```  

For our corpus, currently the number of topics is manually set at 5, so a future step would be determining the best number of topics for the corpus.  

## Sentiment Extraction and Aggregation (Spring Semester)  

<!-- \TeX\ is the best way to typeset mathematics. Donald Knuth designed \TeX\ when he got frustrated at how long it was taking the typesetters to finish his book, which contained a lot of mathematics.  One nice feature of _R Markdown_ is its ability to read LaTeX code directly. -->

<!-- If you are doing a thesis that will involve lots of math, you will want to read the following section. -->

<!-- $$\sum_{j=1}^n (\delta\theta_j)^2 \leq {{\beta_i^2}\over{\delta_i^2 + \rho_i^2}} -->
<!-- \left[ 2\rho_i^2 + {\delta_i^2\beta_i^2\over{\delta_i^2 + \rho_i^2}} \right] \equiv \omega_i^2 -->
<!-- $$ -->

<!-- From Informational Dynamics, we have the following (Dave Braden): -->

<!-- After _n_ such encounters the posterior density for $\theta$ is -->

<!-- $$ -->
<!-- \pi(\theta|X_1< y_1,\dots,X_n<y_n) \varpropto \pi(\theta) \prod_{i=1}^n\int_{-\infty}^{y_i} -->
<!--    \exp\left(-{(x-\theta)^2\over{2\sigma^2}}\right)\ dx -->
<!-- $$ -->

<!-- Another equation: -->

<!-- $$\det\left|\,\begin{matrix}% -->
<!-- c_0&c_1\hfill&c_2\hfill&\ldots&c_n\hfill\cr -->
<!-- c_1&c_2\hfill&c_3\hfill&\ldots&c_{n+1}\hfill\cr -->
<!-- c_2&c_3\hfill&c_4\hfill&\ldots&c_{n+2}\hfill\cr -->
<!-- \,\vdots\hfill&\,\vdots\hfill& -->
<!--   \,\vdots\hfill&&\,\vdots\hfill\cr -->
<!-- c_n&c_{n+1}\hfill&c_{n+2}\hfill&\ldots&c_{2n}\hfill\cr -->
<!-- \end{matrix}\right|>0$$ -->


<!-- Lapidus and Pindar, Numerical Solution of Partial Differential Equations in Science and -->
<!-- Engineering.  Page 54 -->

<!-- $$ -->
<!-- \int_t\left\{\sum_{j=1}^3 T_j \left({d\phi_j\over dt}+k\phi_j\right)-kT_e\right\}w_i(t)\ dt=0, -->
<!--    \qquad\quad i=1,2,3. -->
<!-- $$ -->

<!-- L\&P  Galerkin method weighting functions.  Page 55 -->

<!-- $$ -->
<!-- \sum_{j=1}^3 T_j\int_0^1\left\{{d\phi_j\over dt} + k\phi_j\right\} \phi_i\ dt -->
<!--    = \int_{0}^1k\,T_e\phi_idt, \qquad i=1,2,3 $$ -->

<!-- Another L\&P (p145) -->

<!-- $$ -->
<!-- \int_{-1}^1\!\int_{-1}^1\!\int_{-1}^1 f\big(\xi,\eta,\zeta\big) -->
<!--    = \sum_{k=1}^n\sum_{j=1}^n\sum_{i=1}^n w_i w_j w_k f\big( \xi,\eta,\zeta\big). -->
<!-- $$ -->

<!-- Another L\&P (p126) -->

<!-- $$ -->
<!-- \int_{A_e} (\,\cdot\,) dx dy = \int_{-1}^1\!\int_{-1}^1 (\,\cdot\,) \det[J] d\xi d\eta. -->
<!-- $$ -->


