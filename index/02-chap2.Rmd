# Methodology {#methods}

<!-- Required to number equations in HTML files -->
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  TeX: { equationNumbers: { autoNumber: "AMS" } }
});
</script>  

## Identify Common Topics (Entire Corpus)   
Seeing our corpus as a collection of review documents sharing some common topics, the task of extracting these topics can be framed as a topic modeling one. In particular, we use Latent Dirichlet Allocation (LDA) to extract hidden common topics in restaurant reviews.   

### Data Pre-processing   
Standard text data cleaning procedure was followed for preprocessing the review corpus: all characters were transformed to lowercase, punctuations and numbers removed, stop words such as “I”, “me”, “she”, “is” also removed, whitespaces stripped and trimmed, and all words stemmed using Porter stemming algorithm.

#### N-Gramming  
One optional yet particularly helpful preprocessing step was n-gramming. It captures word sequences that are better perceived or carry more meaningful information as a whole. For example, “White House” should be perceived as a single token instead of separately as “white” and “house”, and similarly “President of the United States” makes more sense as a whole. N-gramming is especially helpful when identifying word sequences that are specific to the context of the corpus. In the previous example, it would be crucial to identify “White House” and “President of the United States” as n-grams in a corpus consists of political blogs. In our case where the corpus is consists of restaurant reviews, we can expect to see context-specific n-grams such as “Mac ‘n Cheese” or “highly recommend”. We take the probabilistic approach for identifying these n-grams where the conditional probability of seeing the i-th word given the (i-1) th word. The n-gramming implementation code was modified based on the NGramming Processing Script (c) by Teague Henry. The procedure in this code is for each consecutive bigram sequence (*$word_1$*, *$word_2$*):   

* Calculate **$n_{word_1}$** = number of occurrences of *$word_1$* in the corpus;    
* Calculate **$p_{word_2}$** = proportion of *$word_2$* in all non-$word_1$ words in the corpus;    
* Calculate **$n_bigram$** = number of occurrences of bigram $word_1\_word_2$ in the corpus;  
* Compute ${p-value} = P(N \geqslant n_{bigram})$ where N is the total number of consecutive co-occurrences of $word_1\_word_2$ where $N \sim Binomial(c_{word_1}, p_{word_2})$;  
* If ${p-value} < 0.01$, then we reject the null hypothesis that the co-occurrences happened by random and identify $word_1\_word_2$ as a meaningful bigram.    

The above procedure is repeated again after first run to identify trigrams and larger n-grams.  

In terms of the cutoff threshold for identifying meaningful n-grams, both count and p-values were considered. While p-value cutoff has comparatively more consistent performance, it alone would include bigrams with neglectable occurrences (eg. appeared only 2 times in the entire corpus) and thus contribute minimal information. As a result, a hybrid cutoff using both a p-value cutoff of 0.01 and empirically-set count cutoffs of 100 for bigrams and 40 for trigrams was adopted for our corpus.  

All identified n-grams will be replaced by an integrated token of the original words in the corpus, where bigrams are connected with “_” in between and trigrams with “.”. For example, after all pre-processing steps and n-gramming, *“White House”* would become *“white_hous”*, and *“Mac ‘n Cheese”* would be *“mac_n.chees”.*  

### Latent Dirichlet Allocation (LDA)  
To identify common topics in our corpus, we will first experiment with the topic modeling approach, or more specifically Latent Dirichlet Allocation (LDA). LDA is a Bayesian generative topic model based on the assumption that each document, as a collection of words, is a mixture of a certain number of topics, and that the occurrence of each word in that document can be attributed to one of its topics. In terms of denotations, the entire corpus is a set of documents $\{D_1, ..., Dm\}$, and the words within a document are denoted as $D_i=\{w_{i1},...,w_{in_i}\}$, $w_ij \in W$ where W is a finite vocabulary set of size $V$. Suppose we assume the entire corpus is a mixture of $K$ topics, the data generative process with LDA is as follows:

1. For each topic $k \in \{1,...,K\}$,  
   a. Draw a topic-word proportion vector $\phi_k \sim Dirichlet(\alpha)$  
2. For each document $D_i$,
   a. Draw a document-topic proportion vector $\theta_i \sim Dirichlet(\beta)$  
   b. For each word $w_{ij}$,  
      i. Draw a topic assignment $z_j \sim Multinomial(\theta_i), z_j \in \{1,...,K\}$  
      ii. Draw a word from this topic $w_{ij} \sim Multinomial(\phi_k), w_{ij} \in \{1,...,V\}$   
      
```{r LDA, fig.cap="Diagram of LDA generative process", echo=FALSE}
knitr::include_graphics(path = "figure/LDA.png")
```  

## Opinion Mining (Restaurant-specific)  
Given a restaurant, we propose the following analysis pipeline:  
1. Extract reviewed aspects and associated text based on all reviews on the restaurant;  
2. Categorize the extracted aspects into topics based on LDA results;  
3. For each aspect, derive a sentiment score based on all associated text for this aspect;  
4. Summarize highlights of the given restaurant based on average sentiment scores of extracted highlights.  

### Aspect Extraction  
For a given text, such as “service was fast, food was pretty good and price is very affordable”, the goal is to extract aspect terms $service$, $food$, and $price$, and their associated opinion $fast$, $pretty\ good$, and $very\ affordable$, respectively.   

Since aspect terms are primarily nouns, the candidate pool for aspects is formed by extracting all noun phrases in the given text. Noun phrases, as opposed to single words, are identified so that n-grams like “vanilla ice cream” can be extracted as a whole. At the same time, it would also make more sense to see $vanilla\ ice\ cream$ and $ice\ cream$ as phrases belong to the same aspect of *“ice cream”* to avoid redundancy. In consideration of both, we lemmatize extracted phrases to the root form ($ice\_cream$ or $cream$) but also keep track of all distinct phrases under this aspect ($vanilla\ ice\ cream$) for potential future use.   

While hundreds of aspects may be mentioned for a restaurant in its review corpus, not all of them are necessarily representative, and thus we only keep aspects mentioned by at least 10% of the reviews for this restaurant. A custom list of stopwords such as “star”, “thing”, and “customer” is also used to filter out noun chunks that are less likely to be meaningful aspects of the restaurant.  

A challenge in the step of aspect extraction is parsing a sentence with multiple aspect terms into chunks corresponding to each aspect. In the example above, for the later precision of aspect-based sentiment score, I only want to attribute “fast” as opposed to other parts or the entire sentence to the specific aspect “service”. To do this, we utilize the common linguistic pattern that different aspects at the same semantic level are usually separated by a comma, “and”, or “but”: we split a mixed sentence according to these conjunctions and attribute relevant parts to concerning aspects.   

### Aspect Categorization  
We experiment with two approaches for categorizing extracted aspects. Assuming some pre-fixed categories based on LDA results or conventions (we use the Annotation Guidelines for SemEval 2016 Task 5: Aspect-based Sentiment Analysis, restaurant domain), we can measure the semantic similarity between an aspect term and each of the categories and assign it to the closest category. The other approach does not make presumptions on categories, but instead try to automatically identify them through clustering.    

Both approaches require some numerical representations of the aspects, and a natural choice would be using word embedding models. Word embeddings map words and phrases to vectors of real numbers using methods such as neural networks (Word2Vec) and dimensionality reduction on the word co-occurrence matrix (GloVe). For our purpose, we use a GloVe model with 300-dimensional vectors pre-trained on 42B tokens from Common Crawl. We use a pre-trained model instead of training our own on the corpus (300M tokens) in consideration of both the computation cost and the quality and size of vocabulary, and we chose the 42B-token version trained on Common Crawl since it has the largest vocab (1.9M) that could fit into the memory we have and is more suitable for our corpus comparing to the 6B-token Wikipedia in terms of the type of expression used.   

For the first approach with pre-fixed categories, we use cosine similarities between each extracted aspect and the categories to determine its category; for the second approach, we use vector representations of extracted aspects to automatically identify clusters.   

### Sentiment Analysis  
After parsing the review corpus for a restaurant into aspect terms and corresponding opinion sentences, we can perform sentiment analysis on the opinion sentences for each aspect term.   

We recognize that both lexicon-based approach and machine learning approach can be used for this purpose. Lexicon-based methods are widely-used in the field for its intuitiveness and ease of implementation, but have limited coverage of lexical features and are not quite adaptive (manually expanding the lexicon is extremely labor-intensive and time-consuming). Machine learning methods, on the other hand, are much more flexible and often lead to better accuracy, but as with high-quality sentiment lexicons for lexicon-based methods, extensive training data are essential to the performance and validity of machine learning methods but often hard to acquire. Additionally, texts in online reviews are often short and sparse, and thus pose a challenge for ensuring the quality and quantity of input features. arse. Practically, machine learning models can also be quite expensive to implement computationally, especially when dealing with an enormous corpus. Last but not least, machine learning methods can become “black boxes” and lose interpretability.   
      
```{r s-a, fig.cap="Sentiment analysis / classificaion techniques", echo=FALSE}
knitr::include_graphics(path = "figure/s-a.png")
```  

Based on the discussion above, we decide to take a primarily lexicon-based approach (more specifically a dictionary-based approach) for the scope of this project. We experimented with two open-source implementations of refined lexicon-based sentiment analysis. One is VADER (Valence Aware Dictionary and sEntiment Reasoner), which combines a sentiment lexicon specifically attuned to social media context with grammatical and syntactical heuristics that capture intensity of sentiment, and the other is TextBlob, which uses a lexicon of adjectives (eg. good, bad, amazing, irritating etc.) that occur frequently in product reviews and takes into account semantic patterns when averaging the polarity for lexical features (eg. words). 
Online reviews do share many common characteristics with social media texts in terms of the distinctive way of expression and relative informality of language. Many of these characteristics, such as the prevalence of slang words, acronyms, and emoticons and the use of all caps and excessive punctuations, though commonly cleaned in the pre-processing step or simply ignored by traditional sentiment analysis tools, may contain valuable information about the polarity and intensity of sentiment. The table below shows a representative sample of challenging cases in sentiment analysis (especially in the social media/ online review context) and a comparison of results from VADER and TextBlob against the baseline of AFINN scores. As expected, VADER is quite sensitive to the intensity as well as binary polarity of sentiment, while TextBlob also has decent performance. The implementations are not without their shortcomes: VADER does seem to have a denser score distribution but at the same time also slightly dramatic changes in score in presence of emoticons and qualifications, and TextBlob tends to ignore slang words and abbreviations, but overall the two lexicon-based methods are giving reasonable results.   

```{r, echo=FALSE}
library(kableExtra)
tbl <- read.csv("data/challenge.csv")
names(tbl) = c("Challenge","Example text", "AFINN", "VADER", "TextBlob")
knitr::kable(tbl, caption = "Common challenges in sentiment analysis and comparison of performances between VADER and TextBlob against baseline results from AFINN score.", "html") %>% kable_styling(bootstrap_options = c("striped", "hover"), full_width = F, font_size = 14) %>% row_spec(c(5,7,11), bold = T)
```   

For each aspect, we aggregate a sentiment score using the sum of polarity scores for all review text associated with this aspect, divided by the number of reviews mentioning this aspect to account for both the volume of opinions expressed towards this aspect and the number of units (reviews) in which they were expressed.  

<!-- \TeX\ is the best way to typeset mathematics. Donald Knuth designed \TeX\ when he got frustrated at how long it was taking the typesetters to finish his book, which contained a lot of mathematics.  One nice feature of _R Markdown_ is its ability to read LaTeX code directly. -->

<!-- If you are doing a thesis that will involve lots of math, you will want to read the following section. -->

<!-- $$\sum_{j=1}^n (\delta\theta_j)^2 \leq {{\beta_i^2}\over{\delta_i^2 + \rho_i^2}} -->
<!-- \left[ 2\rho_i^2 + {\delta_i^2\beta_i^2\over{\delta_i^2 + \rho_i^2}} \right] \equiv \omega_i^2 -->
<!-- $$ -->

<!-- From Informational Dynamics, we have the following (Dave Braden): -->

<!-- After _n_ such encounters the posterior density for $\theta$ is -->

<!-- $$ -->
<!-- \pi(\theta|X_1< y_1,\dots,X_n<y_n) \varpropto \pi(\theta) \prod_{i=1}^n\int_{-\infty}^{y_i} -->
<!--    \exp\left(-{(x-\theta)^2\over{2\sigma^2}}\right)\ dx -->
<!-- $$ -->

<!-- Another equation: -->

<!-- $$\det\left|\,\begin{matrix}% -->
<!-- c_0&c_1\hfill&c_2\hfill&\ldots&c_n\hfill\cr -->
<!-- c_1&c_2\hfill&c_3\hfill&\ldots&c_{n+1}\hfill\cr -->
<!-- c_2&c_3\hfill&c_4\hfill&\ldots&c_{n+2}\hfill\cr -->
<!-- \,\vdots\hfill&\,\vdots\hfill& -->
<!--   \,\vdots\hfill&&\,\vdots\hfill\cr -->
<!-- c_n&c_{n+1}\hfill&c_{n+2}\hfill&\ldots&c_{2n}\hfill\cr -->
<!-- \end{matrix}\right|>0$$ -->


<!-- Lapidus and Pindar, Numerical Solution of Partial Differential Equations in Science and -->
<!-- Engineering.  Page 54 -->

<!-- $$ -->
<!-- \int_t\left\{\sum_{j=1}^3 T_j \left({d\phi_j\over dt}+k\phi_j\right)-kT_e\right\}w_i(t)\ dt=0, -->
<!--    \qquad\quad i=1,2,3. -->
<!-- $$ -->

<!-- L\&P  Galerkin method weighting functions.  Page 55 -->

<!-- $$ -->
<!-- \sum_{j=1}^3 T_j\int_0^1\left\{{d\phi_j\over dt} + k\phi_j\right\} \phi_i\ dt -->
<!--    = \int_{0}^1k\,T_e\phi_idt, \qquad i=1,2,3 $$ -->

<!-- Another L\&P (p145) -->

<!-- $$ -->
<!-- \int_{-1}^1\!\int_{-1}^1\!\int_{-1}^1 f\big(\xi,\eta,\zeta\big) -->
<!--    = \sum_{k=1}^n\sum_{j=1}^n\sum_{i=1}^n w_i w_j w_k f\big( \xi,\eta,\zeta\big). -->
<!-- $$ -->

<!-- Another L\&P (p126) -->

<!-- $$ -->
<!-- \int_{A_e} (\,\cdot\,) dx dy = \int_{-1}^1\!\int_{-1}^1 (\,\cdot\,) \det[J] d\xi d\eta. -->
<!-- $$ -->


