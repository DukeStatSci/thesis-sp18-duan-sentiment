[
["index.html", "Aspect-based Opinion Mining with Yelp Restaurant Reviews Abstract", " Aspect-based Opinion Mining with Yelp Restaurant Reviews Tianlin Duan May 2018 Abstract Opinion mining, the study of collecting and categorizing complex public opinion, is a special focus in text mining and natural language processing. With the widespread use of social media in the digital age, opinion mining has become an increasingly useful tool with applications in different fields. Among these applications, the extraction of sentiment and opinion in user-generated reviews such as product, movie, or restaurant reviews has engaged much interest given its representation of a direct “voice” of customers and the business and social value embedded within. While analyses of voice-of-the-customer (VOC) materials mainly focus on the classification of sentiment polarity at the document level, reviews rarely express a single, consistent sentiment towards the reviewed object or entity, but rather often involve complex, multi-level, and sometimes contradicting sentiments towards multiple aspects of the same entity. A restaurant review, for example, may embody a positive sentiment towards the overall experience, but more specifically a particularly positive view of the service, neutral towards ambience, and negative towards the food. These aspects and their associated sentiments are key to understanding users’ opinion of the reviewed entity and can be of great use in many application scenarios such as personalization. In this project, we identify common topics in restaurant reviews, propose an analysis pipeline to extract a reviewed entity’s representative aspects and their associated sentiment, and discuss the strength and weakness of different approaches towards each task involved. "],
["1-intro.html", "Chapter 1 Introduction", " Chapter 1 Introduction As the primary crowd-sourced review platform, Yelp is a major source of voice-of-the-customer (VOC) materials on local businesses. Recognizing the incredibly rich values embedded in the text, Yelp has made available to the academic community a large sample of the reviews and is constantly updating the publicly available dataset. The dataset is consists of not only reviews but also detailed information about the businesses and users associated with the reviews, as well as check-ins and photos. For the scope of this project, we focus particularly on restaurant reviews, and the 10th version of the dataset provided 3 million such reviews of 51,625 restaurants. As on the platform, each review in the dataset is associated with a numerical score of 1 to 5 for the reviewed restaurant. Many of the current explorations of the Yelp dataset have been focusing on the correspondence between the review and the score, for example, predicting the numerical score with a given review text, assuming that the score is a reasonable proxy of the opinions expressed in the text. This overall score for the business, however, can be too generalized when different aspects of the dining experience were mentioned, and different or sometimes conflicting feelings were expressed. In response to this observation, this project particularly focuses on extracting common topics (categories of aspects) discussed in restaurant reviews with topic modeling, as well as mining a given restaurant’s most-liked or disliked highlights based on its reviews using aspect-based sentiment analysis. "],
["2-methods.html", "Chapter 2 Methodology 2.1 Identify Common Topics (Entire Corpus) 2.2 Opinion Mining (Restaurant-specific)", " Chapter 2 Methodology 2.1 Identify Common Topics (Entire Corpus) Seeing our corpus as a collection of review documents sharing some common topics, the task of extracting these topics can be framed as a topic modeling one. In particular, we use Latent Dirichlet Allocation (LDA) to extract hidden common topics in restaurant reviews. 2.1.1 Data Pre-processing Standard text data cleaning procedure was followed for preprocessing the review corpus: all characters were transformed to lowercase, punctuations and numbers removed, stop words such as “I”, “me”, “she”, “is” also removed, whitespaces stripped and trimmed, and all words stemmed using Porter stemming algorithm. 2.1.1.1 N-Gramming One optional yet particularly helpful preprocessing step was n-gramming. It captures word sequences that are better perceived or carry more meaningful information as a whole. For example, “White House” should be perceived as a single token instead of separately as “white” and “house”, and similarly “President of the United States” makes more sense as a whole. N-gramming is especially helpful when identifying word sequences that are specific to the context of the corpus. In the previous example, it would be crucial to identify “White House” and “President of the United States” as n-grams in a corpus consists of political blogs. In our case where the corpus is consists of restaurant reviews, we can expect to see context-specific n-grams such as “Mac ‘n Cheese” or “highly recommend”. We take the probabilistic approach for identifying these n-grams where the conditional probability of seeing the i-th word given the (i-1) th word. The n-gramming implementation code was modified based on the NGramming Processing Script (c) by Teague Henry. The procedure in this code is for each consecutive bigram sequence (\\(word_1\\), \\(word_2\\)): Calculate \\(n_{word_1}\\) = number of occurrences of \\(word_1\\) in the corpus; Calculate \\(p_{word_2}\\) = proportion of \\(word_2\\) in all non-\\(word_1\\) words in the corpus; Calculate \\(n_bigram\\) = number of occurrences of bigram \\(word_1\\_word_2\\) in the corpus; Compute \\({p-value} = P(N \\geqslant n_{bigram})\\) where N is the total number of consecutive co-occurrences of \\(word_1\\_word_2\\) where \\(N \\sim Binomial(c_{word_1}, p_{word_2})\\); If \\({p-value} &lt; 0.01\\), then we reject the null hypothesis that the co-occurrences happened by random and identify \\(word_1\\_word_2\\) as a meaningful bigram. The above procedure is repeated again after first run to identify trigrams and larger n-grams. In terms of the cutoff threshold for identifying meaningful n-grams, both count and p-values were considered. While p-value cutoff has comparatively more consistent performance, it alone would include bigrams with neglectable occurrences (eg. appeared only 2 times in the entire corpus) and thus contribute minimal information. As a result, a hybrid cutoff using both a p-value cutoff of 0.01 and empirically-set count cutoffs of 100 for bigrams and 40 for trigrams was adopted for our corpus. All identified n-grams will be replaced by an integrated token of the original words in the corpus, where bigrams are connected with “_” in between and trigrams with “.”. For example, after all pre-processing steps and n-gramming, “White House” would become “white_hous”, and “Mac ‘n Cheese” would be “mac_n.chees”. 2.1.2 Latent Dirichlet Allocation (LDA) To identify common topics in our corpus, we will first experiment with the topic modeling approach, or more specifically Latent Dirichlet Allocation (LDA). LDA is a Bayesian generative topic model based on the assumption that each document, as a collection of words, is a mixture of a certain number of topics, and that the occurrence of each word in that document can be attributed to one of its topics. In terms of denotations, the entire corpus is a set of documents \\(\\{D_1, ..., Dm\\}\\), and the words within a document are denoted as \\(D_i=\\{w_{i1},...,w_{in_i}\\}\\), \\(w_ij \\in W\\) where W is a finite vocabulary set of size \\(V\\). Suppose we assume the entire corpus is a mixture of \\(K\\) topics, the data generative process with LDA is as follows: For each topic \\(k \\in \\{1,...,K\\}\\), Draw a topic-word proportion vector \\(\\phi_k \\sim Dirichlet(\\alpha)\\) For each document \\(D_i\\), Draw a document-topic proportion vector \\(\\theta_i \\sim Dirichlet(\\beta)\\) For each word \\(w_{ij}\\), Draw a topic assignment \\(z_j \\sim Multinomial(\\theta_i), z_j \\in \\{1,...,K\\}\\) Draw a word from this topic \\(w_{ij} \\sim Multinomial(\\phi_k), w_{ij} \\in \\{1,...,V\\}\\) Figure 2.1: Diagram of LDA generative process 2.2 Opinion Mining (Restaurant-specific) Given a restaurant, we propose the following analysis pipeline: 1. Extract reviewed aspects and associated text based on all reviews on the restaurant; 2. Categorize the extracted aspects into topics based on LDA results; 3. For each aspect, derive a sentiment score based on all associated text for this aspect; 4. Summarize highlights of the given restaurant based on average sentiment scores of extracted highlights. 2.2.1 Aspect Extraction For a given text, such as “service was fast, food was pretty good and price is very affordable”, the goal is to extract aspect terms \\(service\\), \\(food\\), and \\(price\\), and their associated opinion \\(fast\\), \\(pretty\\ good\\), and \\(very\\ affordable\\), respectively. Since aspect terms are primarily nouns, the candidate pool for aspects is formed by extracting all noun phrases in the given text. Noun phrases, as opposed to single words, are identified so that n-grams like “vanilla ice cream” can be extracted as a whole. At the same time, it would also make more sense to see \\(vanilla\\ ice\\ cream\\) and \\(ice\\ cream\\) as phrases belong to the same aspect of “ice cream” to avoid redundancy. In consideration of both, we lemmatize extracted phrases to the root form (\\(ice\\_cream\\) or \\(cream\\)) but also keep track of all distinct phrases under this aspect (\\(vanilla\\ ice\\ cream\\)) for potential future use. While hundreds of aspects may be mentioned for a restaurant in its review corpus, not all of them are necessarily representative, and thus we only keep aspects mentioned by at least 10% of the reviews for this restaurant. A custom list of stopwords such as “star”, “thing”, and “customer” is also used to filter out noun chunks that are less likely to be meaningful aspects of the restaurant. A challenge in the step of aspect extraction is parsing a sentence with multiple aspect terms into chunks corresponding to each aspect. In the example above, for the later precision of aspect-based sentiment score, I only want to attribute “fast” as opposed to other parts or the entire sentence to the specific aspect “service”. To do this, we utilize the common linguistic pattern that different aspects at the same semantic level are usually separated by a comma, “and”, or “but”: we split a mixed sentence according to these conjunctions and attribute relevant parts to concerning aspects. 2.2.2 Aspect Categorization We experiment with two approaches for categorizing extracted aspects. Assuming some pre-fixed categories based on LDA results or conventions (we use the Annotation Guidelines for SemEval 2016 Task 5: Aspect-based Sentiment Analysis, restaurant domain), we can measure the semantic similarity between an aspect term and each of the categories and assign it to the closest category. The other approach does not make presumptions on categories, but instead try to automatically identify them through clustering. Both approaches require some numerical representations of the aspects, and a natural choice would be using word embedding models. Word embeddings map words and phrases to vectors of real numbers using methods such as neural networks (Word2Vec) and dimensionality reduction on the word co-occurrence matrix (GloVe). For our purpose, we use a GloVe model with 300-dimensional vectors pre-trained on 42B tokens from Common Crawl. We use a pre-trained model instead of training our own on the corpus (300M tokens) in consideration of both the computation cost and the quality and size of vocabulary, and we chose the 42B-token version trained on Common Crawl since it has the largest vocab (1.9M) that could fit into the memory we have and is more suitable for our corpus comparing to the 6B-token Wikipedia in terms of the type of expression used. For the first approach with pre-fixed categories, we use cosine similarities between each extracted aspect and the categories to determine its category; for the second approach, we use vector representations of extracted aspects to automatically identify clusters. 2.2.3 Sentiment Analysis After parsing the review corpus for a restaurant into aspect terms and corresponding opinion sentences, we can perform sentiment analysis on the opinion sentences for each aspect term. We recognize that both lexicon-based approach and machine learning approach can be used for this purpose. Lexicon-based methods are widely-used in the field for its intuitiveness and ease of implementation, but have limited coverage of lexical features and are not quite adaptive (manually expanding the lexicon is extremely labor-intensive and time-consuming). Machine learning methods, on the other hand, are much more flexible and often lead to better accuracy, but as with high-quality sentiment lexicons for lexicon-based methods, extensive training data are essential to the performance and validity of machine learning methods but often hard to acquire. Additionally, texts in online reviews are often short and sparse, and thus pose a challenge for ensuring the quality and quantity of input features. arse. Practically, machine learning models can also be quite expensive to implement computationally, especially when dealing with an enormous corpus. Last but not least, machine learning methods can become “black boxes” and lose interpretability. Figure 2.2: Sentiment analysis / classificaion techniques Based on the discussion above, we decide to take a primarily lexicon-based approach (more specifically a dictionary-based approach) for the scope of this project. We experimented with two open-source implementations of refined lexicon-based sentiment analysis. One is VADER (Valence Aware Dictionary and sEntiment Reasoner), which combines a sentiment lexicon specifically attuned to social media context with grammatical and syntactical heuristics that capture intensity of sentiment, and the other is TextBlob, which uses a lexicon of adjectives (eg. good, bad, amazing, irritating etc.) that occur frequently in product reviews and takes into account semantic patterns when averaging the polarity for lexical features (eg. words). Online reviews do share many common characteristics with social media texts in terms of the distinctive way of expression and relative informality of language. Many of these characteristics, such as the prevalence of slang words, acronyms, and emoticons and the use of all caps and excessive punctuations, though commonly cleaned in the pre-processing step or simply ignored by traditional sentiment analysis tools, may contain valuable information about the polarity and intensity of sentiment. The table below shows a representative sample of challenging cases in sentiment analysis (especially in the social media/ online review context) and a comparison of results from VADER and TextBlob against the baseline of AFINN scores. As expected, VADER is quite sensitive to the intensity as well as binary polarity of sentiment, while TextBlob also has decent performance. The implementations are not without their shortcomes: VADER does seem to have a denser score distribution but at the same time also slightly dramatic changes in score in presence of emoticons and qualifications, and TextBlob tends to ignore slang words and abbreviations, but overall the two lexicon-based methods are giving reasonable results. Table 2.1: Common challenges in sentiment analysis and comparison of performances between VADER and TextBlob against baseline results from AFINN score. Challenge Example text AFINN VADER TextBlob Punctuation (intensify) Food was good!!! 0.6 0.58 1.0 Word shape (intensify) Food was GOOD. 0.6 0.56 0.7 Emoticon Food was good :) 0.6 0.71 0.6 Degree modifier Food was very good. 0.6 0.49 0.9 Baseline (positive) Food was good. 0.6 0.44 0.7 Negation I did not dislike the food. -0.4 0.29 0.0 Baseline (neutral) Food was okay. 0.0 0.23 0.5 Qualification (mild negative) Food was okay but I would not recommend it to you. 0.4 -0.30 0.5 Contraction as negation Food was not very good. 0.6 -0.39 -0.3 Slang word Food sux. 0.0 -0.36 0.0 Baseline (negative) Food was bad. -0.6 -0.54 -0.7 For each aspect, we aggregate a sentiment score using the sum of polarity scores for all review text associated with this aspect, divided by the number of reviews mentioning this aspect to account for both the volume of opinions expressed towards this aspect and the number of units (reviews) in which they were expressed. "],
["3-results.html", "Chapter 3 Results 3.1 Exploratory Data Analysis 3.2 Common Topics (LDA) 3.3 Restaurant Opinion Mining", " Chapter 3 Results 3.1 Exploratory Data Analysis Before diving into the analysis of the corpus, we shall first explore the distribution and characteristics of the reviews and the reviewed restaurants. Since our corpus is a not-necessarily random sample of Yelp reviews, the insights below only may not reflect the true distribution or characteristics of the reviewed restaurants or reviews on Yelp. Figure 3.1: Geo-distribution of reviewed restaurant (showing top 10 locations). As shown in the graph above, the top 10 geolocations account for 95% of all reviewed restaurants in our dataset: more than half (57.6%) of the reviewed restaurants in our dataset are located in Ontario (Canada), Arizona, and Nevada, and another 31% of restaurants about evenly distributed in Quebec, Ohio, North Carolina. While Ontario and Quebec, Canada do have two of the most number of reviews, all or the vast majority of these views are written in English and thus would not impact the results for later analysis. Figure 3.2: Distribution of length of reviews in dataset. The average length of reviews is at 111 words and the most common length is around 50 words, typical of the the “microblog” nature of online text, but we do have an extremely long right-tail indicating a small proportion of quite lengthy reviews, with about 6% of all reviews spanning from 300 to 1,000 words. Figure 3.3: Distribution of star-ratings by cuisine category / restaurant type. Figure 3.4: Percentage of star-ratings by cuisine category / restaurant type. Categories sorted in descending order of total number of reviews. In terms of distribution of star ratings, the entire dataset is consists of 37% of reviews with a 5-star rating, 28% with 4 stars, 14% with 3, 10% with 2, and 11%. The distribution for each Cuisine Category / Restaurant Type, while mostly consistent with that of the entire dataset, does show some interesting scoring differences between fine-dining and casual eating scenarios: assuming our dataset random and unbiased, Vegetarian, Steakhouse, Seafood, Sandwiches restaurants tend to receive higher percentage of 5-star ratings, while Chinese, Burgers, Pizza, and Mexican restaurants have significantly higher percentage of 1-star ratings. Figure 3.5: Word cloud of 50 most frequent words in corpus (pre-stemming). We can already see many potential “aspects” and associated “sentiment” that we would like to extract next. 3.2 Common Topics (LDA) 3.2.1 N-Gramming Using the n-gramming method with hybrid cutoff thresholds discussed in the Methodology section, 2148 bigrams and 406 trigrams (and larger n-grams) were identified. Below shows the top and most representative n-grams identified in our corpus, and it is evident that the method successfully identified context-specific phrases that should be processed as single tokens. Table 3.1: Top context-specific bigrams (stemmed). Bigram p-value go_back 0 high_recommend 0 las_vega 7.6e-201 great_food 1.06e-75 happi_hour 1.19e-75 custom_servic 2.73e-26 Table 3.2: Top context-specific trigrams (stemmed). Trigram p-value cant.go_wrong 0 mac_n.chees 0 sweet_potato.fri 0 seat.right_away 7.16e-228 kung_pao.chicken 8.85e-119 fast_food.chain 1.24e-79 3.2.2 LDA While no “ground truth” exist for topic modeling results, we may expect to see category-aspect pairs such as Food / Drinks (taste, portion, price, options), Service (timeliness, friendliness), Ambience (noise level, occasion), Location, and Restaurant in general (cuisine category, restaurant type) etc.. We check if these targeted aspects can be recovered automatically. Table 3.3: LDA topics and associated top terms (entire corpus, stemmed) Topic 1 Topic 2 Topic 3 Topic 4 Topic 5 place burger just restaur tabl locat chicken pizza delici order sandwich sauc get meal ask day flavor eat salad us bar fri like menu said alway order place amaz server beer tast think dinner servic love good im steak time friend meat price great went great like realli bread one When trained on the entire corpus, LDA identified topics that are in general consistent with the expectation above: Restaurant (Ambience and Location) (Topic 1), Food-quality (Topic 2, more casual or lunch style), Food-price / dining experience (Topic 3), Food-options / quality (Topic 4, fine-dining or dinner style), Service (Topic 5). The food-related topics, however, do seem to contain tokens representing a mix of aspects (price, taste, variety). In other words, the food topics need more discriminating differentiation. One potential explanation for this drawback could be the various but unevenly distributed cuisine categories we have in the corpus: while service and restaurant ambience / location may be shared topics containing similar tokens regardless of the cuisine type, food-related topics and associated tokens are likely to be more category-specific or even restaurant-specific. In response, we experiment with running LDA on segmented corpus for each cuisine category. Table 3.4: LDA topics and associated top terms (restaurants with category ‘Korean’, stemmed) Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 spici sauc meat like korean bulgogi side flavor just restaur soup bbq chicken even place dish improp great server delici hot good food one korean_food rice tri littl come sushi tofu pull_pork tast us make stone pig well tabl best pork nice eat order lot bowl rib taco time authent When trained on corpus segmented by cuisine category, Food topic and corresponding top terms become much more category-specific. As illustrated in the results from Korean category corpus, specialty-food-related tokens are extracted and grouped into topics (Bibimbap-related in Topic 1, Korean BBQ-related in Topic 2) in addition to more general topics seen in the entire corpus (Food-quality in Topic 3, Service in Topic 4, Restaurant in Topic 5). 3.3 Restaurant Opinion Mining 3.3.1 Aspect Extraction Unlike the extraction of common topics, our analysis of highlights (aspects) and corresponding sentiment is specific to each restaurant. To illustrate the opinion mining process, we use results for a randomly picked restaurant (Kitchen M in Markham, Ontario). Based on 73 reviews on this restaurant, 32 aspect terms were extracted (in order of most mentioned, by number of reviews): \\[ \\begin{aligned} food,\\ place,\\ service,\\ dish,\\ restaurant,\\ menu,\\ rice,\\ soup,\\ tea,\\ table,\\\\ meal,\\ cafe,\\ price,\\ sauce,\\ drink,\\ portion,\\ noodle,\\ staff,\\ congee,\\\\ lunch,\\ option,\\ spaghetti,\\ breakfast,\\ dessert,\\ waitress,\\\\ cheese,\\ area,\\ style,\\ order,\\ space,\\ sandwich,\\ steak \\end{aligned} \\] 3.3.2 Aspect Categorization We previously discussed two approaches for categorizing extracted aspect terms: one is assigning each term to the closest pre-defined category based on cosine similarity, and the other is automatically identify them with clustering without making presumptions about the categories. For the first approach, we decided not to use the pre-defined categories of SemEval (Food, Drinks, Service, Ambience, Location, Restaurant) but rely on the LDA results for our corpus (Food, Service, Restaurant) since the latter is consistent with the former but more representative of our corpus. The results, as shown below, are not especially satisfying. Each category has a few aspects improperly assigned, most significantly in the Restaurant category, where a collection of words (‘noodle’, ‘congee’, ‘waitress’) bearing clear categories were wrongly assigned. With a closer examination, we realized that for many words, their cosine similarity to different pre-defined categories can be quite similar, and thus the assignments were made with little confidence. In addition, regardless of categories, extracted aspects all belong to the larger domain of restaurant characteristics, and thus are semantically close by nature. Table 3.5: Categorization results using cosine similarity between aspect terms and each pre-fixed category terms. Category Aspect terms assigned to this category Food ‘spaghetti’, ‘sauce’, ‘dish’, ‘soup’, ‘drink’, ‘food’, ‘portion’, ‘table’, ‘rice’, ‘tea’, ‘place’, ‘lunch’, ‘sandwich’, ‘meal’, ‘dessert’, ‘cheese’ Service ‘option’, ‘price’, ‘area’, ‘staff’, ‘service’, ‘order’, ‘space’ Restaurant ‘menu’, ‘style’, ‘cafe’, ‘restaurant’, ‘breakfast’, ‘noodle’, ‘congee’, ‘waitress’, ‘steak’ For the second approach, we cluster the extracted aspects using their word embedding representations based on the 300-dimensional, 42B pre-trained GloVe model. The result shows that the aspects can be approximately grouped into 3 clusters of dishes, food &amp; drink types and dining experience attributes, generally consistent with the LDA results, but is also lacking more discriminating differentiation within topics. Figure 3.6: t-SNE visualization of clustering results for extracted aspects using vector representations from GloVe. Since neither approach derived very satisfying result for categorization, we decide to focus on the aspect level for sentiment analysis, and postpone the aggregation of categorical scores for the scope of this project. 3.3.3 Sentiment Analysis In the Methodology section, we discussed two refined lexicon-based sentiment analysis implementations, VADER and TextBlob. While they had decent performance on common challenging use cases, it is worthwhile to do a sanity check on whether they can capture the overall sentiments in our review corpus. Using the overall star rating as proxy for review polarity, we perform sentiment analysis on all reviews in the corpus and compare the two tools’ ability to recover the general polarity of reviews. From the graphs below, it is evident that polarity scores derived by TextBlob have better correlation with the star ratings and thus may be more suitable for our sentiment analysis task. Figure 3.7: Sanity check with candidate sentiment analysis tools. VADER, while attuned to distinctive expression patterns in social media context and sensitive to intensity of polarity, may focus much on the nuances but lose grip on the overall polarity. TextBlob, on the other hand, shows good correlation with the actual star ratings. Figure 3.7: Sanity check with candidate sentiment analysis tools. VADER, while attuned to distinctive expression patterns in social media context and sensitive to intensity of polarity, may focus much on the nuances but lose grip on the overall polarity. TextBlob, on the other hand, shows good correlation with the actual star ratings. TextBlob’s better performance was further confirmed by results for individual review text. For each of the extracted aspects of a given restaurant, all review text concerning the aspect are collected. VADER and TextBlob are then used to perform sentiment analysis on each of the review text. The table below shows all review text in the corpus for our example restaurant concerning the aspect ‘price’ along with the corresponding polarity scores derived by both tools. Table 3.6: Review text on ‘price’ for example restaurant and corresponding scores by VADER and TextBlob. VADER seems to have less lexical features (words) in its vocab and thus missed many sentiment keywords. TextBlob captured the majority of sentiment words possibly due to its lexicon attuned to product reviews, but also had problems with context-dependent sentiments such as “high (price)” and “above average (price)”. Review text concerning ‘price’ VADER score TextBlob score ‘the prices are fairly good’ 0.44 0.70 ‘price is very affordable’ 0.00 0.20 ‘price are very reasonable’ 0.00 0.26 ‘their price is reasonable’ 0.00 0.20 ‘Prices are reasonable’ 0.00 0.20 ‘The price seems above average’ 0.00 -0.08 ‘high prices’ 0.00 0.16 “Kitchen M doesn’t charges high prices” 0.21 0.16 ‘they gave you decent food for relatively good price’ 0.44 0.43 ‘Another plus is the affordability; you get enormous portions for Scarborough prices’ 0.00 0.00 ‘The prices are cheap here’ 0.00 0.40 ‘the price is a little too high for this type of food’ 0.00 -0.01 “you really can’t expect much for the price you pay” -0.10 0.20 ‘good price’ 0.44 0.70 ‘dinner specials for a good price’ 0.44 0.70 ‘Very filling for a good price’ 0.49 0.45 From the results above, it is evident that words bearing context-dependent sentiment impose a challenge for lexicon-based methods in general. In the examples above, cheap and affordable should carry positive sentiment while high carries negative when associated with price, but the words themselves do not necessarily have a polarity, and can even carry opposite polarity when associated with different context (eg. high price is negative but high score can be positive). This limitation of lexicon-based method is hard to resolve, since the unit of analysis in these approach is often words, and thus even with the incorporation of semantic patterns only limited of info can be incorporated from the context. The final step in our analysis is to derive an averaged score for each aspect and summarize the highlights (most-mentioned and most-positively-reviewed) of the given restaurant. Below is the summary for the example restaurant. Interestingly, our summarized highlights do overlap with the ones provided by Yelp for Kitchen M (our example restaurant), though Yelp seems to focus more on the “most-discussed” part when defining highlights and gives little explicit attention to the sentiment associated. Table 3.7: Summary of highlights for the example restaurant. Highlight Representative review text Price (0.27) “price are very reasonable” Portion (0.24) “The dishes I’ve ever ordered from the restaurant always come in a large portion” Food (0.23) “food here is okay” Tea (0.19) “The Hong Kong style milk tea was also great” Place (0.14) “It’s a nice HK cafe kind of place” Figure 3.8: Screenshot of Yelp-provided highlights for the example restaurant. These highlights, though selected mainly based on number of reviews mentioning it, are not necessarily the most-positively-reviewed. Interestingly, these aspects do overlap with the ones extracted with our sentiment-focused approach. "],
["4-conclusion.html", "Chapter 4 Conclusion &amp; Future Work", " Chapter 4 Conclusion &amp; Future Work LDA identified Food (quality, options) , Service, and Restaurant as the commonly discussed topics in Yelp restaurant reviews, but distinctions among topics are not especially clear; Next step is to optimize hyper-parameters including number of topics. POS tagging and semantic patterns gave decent results in extracting meaningful aspects from reviews for a given restaurant, but have limited ability in handling complex sentences with mixed aspects; Next step is to experiment with methods that incorporate more semantic context, such as dependency parsing. Word embedding representations can provide preliminary results on categorizing extracted aspects, but as with LDA topics, aspects in the same domain (restaurant) are by nature semantically close and thus pose challenge to effective clustering/categorization; Next step is to explore better options for aspect categorization. Lexicon (dictionary)-based approach, when combined with semantic patterns or rules, have overall decent performance on sentiment analysis, but is still constrained by word-level information and thus can only reach limited accuracy; Next step is to experiment with machine-learning based approach for sentiment analysis. The original motivation and also a particularly meaningful application of this project is to provide personalized restaurant recommendations based on a user’s review. A potential approach to this task would be Doc2Vec, which can measure the distance from a given text (can be a word, a phrase, a sentence, or a full review text) to the documents (reviews) in corpus. With improved aspect extraction and sentiment analysis as well as categorization methods, it would also be interesting to derive categorical scores (for food, service, ambience, location, price etc.) and use them as features to explain and predict users’ overall sentiment towards a restaurant — what category or aspect has the most impact? Does the answer differ for different cuisine categories / restaurant types? Such incredibly rich dataset allows us to experiment with a wide range of tasks in the field of Text Mining and Natural Language Processing. This project is a starting point for these exploration and I look forward to the learning experience (and fun) ahead. If you feel it necessary to include an appendix, it goes here. --> "],
["A-lda-result-by-cuisine-category.html", "A LDA Result by Cuisine Category", " A LDA Result by Cuisine Category Table A.1: LDA top terms American Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 burger just tabl restaur bar fri get server delici beer order go ask dinner place sandwich make us meal drink salad come servic amaz friend like think said perfect locat got food order steak great good one even dish night littl im waitress appet lot bread eat manag great menu tri place one dessert awesom came thing went flavor visit chicken can back plate well chees realli seat menu stop top breakfast wait well also Table A.1: LDA top terms Breakfast Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 breakfast menu white.castl tabl taco egg sandwich burger order salsa wait also slider ask great coffe restaur locat servic salsa_bar pancak friend fri said delici place food eat just flavor bacon meal get busi amaz side even just peopl love good time chees us one potato lunch one server tri matt option tast food place morn salad like work well toast eat expect go good like brunch line want best perfect alway better look chip Table A.1: LDA top terms Burgers Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 just place burger order white.castl even good fri ask slider get also bun said locat make go got time one can nice delici manag vega think friend love went line want menu shake wait strip way well sauc drink tast made food flavor tabl eat say bar top work littl im alway meat one onion much will amaz servic like like stop tri busi chees still come fresh back open thing wing dog us better Table A.1: LDA top terms Chinese Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 one chicken chines dish order place sauc sushi beef food dim_sum fri_rice price dumpl eat time soup chines_food shrimp just tabl great best noodl even serv love restaur flavor ask us get take tast went servic nice crave good come go meal place spici realli got littl tri pork said see rice authent restaur want locat like dinner like will owner alway menu also know busi general better bit buffet around fresh china cook thing Table A.1: LDA top terms Italian Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 great get tabl pasta pizza menu go order bread top love place us meal crust delici restaur ask salad chees nice make server italian slice amaz eat drink dish sauc food can time chicken fresh enjoy bar said also like perfect will one serv wing friend locat servic came salad dinner like manag sauc tomato best way waiter meatbal sausag wine im wait order vegan awesom well look side vega dessert just check good garlic Table A.1: LDA top terms Japanese Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 japanes sushi place tabl good ramen roll like restaur rice menu hana one just order tri fish get server chicken delici fresh go us hibachi order sashimi come came sauc littl love realli time got flavor salmon think friend bowl serv price can ask dinner meal great eat servic better dish byob will two lot drink amaz way wait shrimp food alway much experi salad sake phoenix make even use well best want busi piec Table A.1: LDA top terms Korean Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 spici sauc meat like korean bulgogi side flavor just restaur soup bbq chicken even place dish improp great server delici hot good food one korean_food rice tri littl come sushi tofu pull_pork tast us make stone pig well tabl best pork nice eat order lot bowl rib taco time authent came chees get said meal lunch brisket got will alway love grit menu im mani kimchi sweet_potato thing friend enough also went still want dinner Table A.1: LDA top terms Mexican Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 place order one taco burrito good food just salsa chip mexican ask get salsa_bar chicken margarita drink go delici meat shrimp tabl eat tri order friend went like amaz tast come servic make great sauc mexican_food restaur even best nice food said locat horchata chees love time can flavor tortilla authent wait im love plate lunch busi will quesadilla top stop us peopl differ got visit server much favorit bean enjoy menu think fresh guacamol Table A.1: LDA top terms Pizza Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 order beer pizza one salad ask can crust just restaur tabl friend sauc get delici said tri top like menu time place chees eat great call bar slice go nice got enjoy wing place pasta busi alway vegan locat good us drink order realli bread food staff fresh think perfect went also garlic im italian back awesom tast well love wait love pepperoni come amaz told littl vega will dinner manag servic sausag price serv Table A.1: LDA top terms Sandwiches Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 salad burger beer ask sandwich love order place said bread delici good pizza order fri soup got can servic eat great littl drink time locat lunch side friend want one amaz flavor bar look chees day came get busi meat option sauc alway tabl sub food tri like even pittsburgh also nice also back like favorit well ive us realli work tast select just just bacon chicken even manag way perfect serv brew never turkey Table A.1: LDA top terms Seafood Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 dinner get server great shrimp order just us oyster food nice place servic amaz vega littl like tabl love locat meat eat ask bar good serv one time friend beer restaur realli even drink come appet look experi seafood restaur salad say made fresh fri sauc much manag crab menu side im said best fish tast think waitress roll place top peopl first happi.hour lot dish expect want delici yard_hous dessert still never tri chicken Table A.1: LDA top terms Steakhouse Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 order server amaz just steak love us food place cut salad tabl vega get side delici ask well meat bread good restaur menu eat flavor great time made like tast also manag servic realli steakhous dinner one everyth come dessert bar waiter best price ribey appet said locat even filet dish servic great much oz perfect night make think good cocktail never awesom disappoint nice came first recommend better like lobster drink staff one beef Table A.1: LDA top terms Sushi Bars Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 one get great sushi good order place hana roll rice restaur like also fish dish even eat love fresh order tabl realli well sashimi got time just tea happi.hour chicken server think nice salmon flavor us much japanes amaz sauc ask im alway favorit bowl servic better will go small two way menu tempura tast just still can delici spici waitress make tri special meal said look enjoy wasabi hot never ive spot littl lunch Table A.1: LDA top terms Thai Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 curri like thai order dish good eat pad_thai restaur spici rice one place ask soup fri_rice sushi thai_food got flavor nice just delici menu noodl small realli love lunch chicken green well great came littl sweet time tri tabl beef food roll best say spice thai_restaur even hot look tom price come tea take fresh lunch_special think ive meal salad get im favorit sinc serv also never alway said can authent crab basil food shrimp Table A.1: LDA top terms Vegetarian Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 food one soup taco love menu tabl chicken salsa vegan place go fresh salsa_bar also locat just salad amaz good drink time nice like delici servic got dish tri sandwich well order like flavor eat visit came veggi horchata flavor server went get delici im beer ask sauc one order enjoy think serv chip option great us even burrito day price need vegetarian place lunch dinner busi though great healthi experi restaur meat mole salad Table A.1: LDA top terms Vietnamese Topic.1 Topic.2 Topic.3 Topic.4 Topic.5 soup vietnames order noodl pho place restaur broth dish bun flavor just bowl sauc tri like one tast roll want littl menu meat pork eat clean good servic love much even ive go fish asian get also veggi beef side shrimp great im spring_roll right water seem said fresh beef lot special us rice walk chicken come usual sweet tabl meal best ever friend price say far use serv overal around still alway food free "],
["references.html", "References", " References "]
]
